{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "E: Unable to locate package libavdevice-dev\n",
      "E: Unable to locate package libavfilter-dev\n"
     ]
    }
   ],
   "source": [
    "!apt install libavdevice-dev libavfilter-dev libopus-dev libvpx-dev pkg-config\n",
    "#!pip install aiohttp aiortc opencv-python websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt install libavdevice-dev libavfilter-dev libopus-dev libvpx-dev pkg-config\n",
    "!pip install aiohttp aiortc opencv-python websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import aiohttp\n",
    "import cv2\n",
    "import websockets\n",
    "from av import VideoFrame\n",
    "\n",
    "from aiortc import (RTCIceCandidate, RTCPeerConnection, RTCSessionDescription,\n",
    "                    VideoStreamTrack)\n",
    "from aiortc.contrib.media import MediaBlackhole, MediaPlayer, MediaRecorder\n",
    "from aiortc.contrib.signaling import object_from_string, object_to_string\n",
    "\n",
    "ROOT = os.path.dirname('.')\n",
    "print(ROOT)\n",
    "PHOTO_PATH = os.path.join(ROOT, 'photo.jpg')\n",
    "\n",
    "\n",
    "class ApprtcSignaling:\n",
    "    def __init__(self, room):\n",
    "        self._http = None\n",
    "        self._origin = 'https://appr.tc'\n",
    "        self._room = room\n",
    "        self._websocket = None\n",
    "\n",
    "    async def connect(self):\n",
    "        join_url = self._origin + '/join/' + self._room\n",
    "\n",
    "        # fetch room parameters\n",
    "        self._http = aiohttp.ClientSession()\n",
    "        async with self._http.post(join_url) as response:\n",
    "            # we cannot use response.json() due to:\n",
    "            # https://github.com/webrtc/apprtc/issues/562\n",
    "            data = json.loads(await response.text())\n",
    "        assert data['result'] == 'SUCCESS'\n",
    "        params = data['params']\n",
    "\n",
    "        self.__is_initiator = params['is_initiator'] == 'true'\n",
    "        self.__messages = params['messages']\n",
    "        self.__post_url = self._origin + '/message/' + self._room + '/' + params['client_id']\n",
    "\n",
    "        # connect to websocket\n",
    "        self._websocket = await websockets.connect(params['wss_url'], extra_headers={\n",
    "            'Origin': self._origin\n",
    "        })\n",
    "        await self._websocket.send(json.dumps({\n",
    "            'clientid': params['client_id'],\n",
    "            'cmd': 'register',\n",
    "            'roomid': params['room_id'],\n",
    "        }))\n",
    "\n",
    "        return params\n",
    "\n",
    "    async def close(self):\n",
    "        if self._websocket:\n",
    "            await self.send(None)\n",
    "            self._websocket.close()\n",
    "        if self._http:\n",
    "            await self._http.close()\n",
    "\n",
    "    async def receive(self):\n",
    "        if self.__messages:\n",
    "            message = self.__messages.pop(0)\n",
    "        else:\n",
    "            message = await self._websocket.recv()\n",
    "            message = json.loads(message)['msg']\n",
    "        print('<', message)\n",
    "        return object_from_string(message)\n",
    "\n",
    "    async def send(self, obj):\n",
    "        message = object_to_string(obj)\n",
    "        print('>', message)\n",
    "        if self.__is_initiator:\n",
    "            await self._http.post(self.__post_url, data=message)\n",
    "        else:\n",
    "            await self._websocket.send(json.dumps({\n",
    "                'cmd': 'send',\n",
    "                'msg': message,\n",
    "            }))\n",
    "\n",
    "class VideoTransformTrack(VideoStreamTrack):\n",
    "    def __init__(self, track, transform):\n",
    "        super().__init__()  # don't forget this!\n",
    "        self.counter = 0\n",
    "        self.track = track\n",
    "        self.transform = transform\n",
    "\n",
    "    async def recv(self):\n",
    "        frame = await self.track.recv()\n",
    "        self.counter += 1\n",
    "\n",
    "        if self.transform == 'cartoon':\n",
    "            img = frame.to_ndarray(format='bgr24')\n",
    "\n",
    "            # prepare color\n",
    "            img_color = cv2.pyrDown(cv2.pyrDown(img))\n",
    "            for _ in range(6):\n",
    "                img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n",
    "            img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n",
    "\n",
    "            # prepare edges\n",
    "            img_edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img_edges = cv2.adaptiveThreshold(\n",
    "                cv2.medianBlur(img_edges, 7), 255,\n",
    "                cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                cv2.THRESH_BINARY, 9, 2)\n",
    "            img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # combine color and edges\n",
    "            img = cv2.bitwise_and(img_color, img_edges)\n",
    "\n",
    "            # rebuild a VideoFrame, preserving timing information\n",
    "            new_frame = VideoFrame.from_ndarray(img, format='bgr24')\n",
    "            new_frame.pts = frame.pts\n",
    "            new_frame.time_base = frame.time_base\n",
    "            return new_frame\n",
    "        elif self.transform == 'edges':\n",
    "            # perform edge detection\n",
    "            img = frame.to_ndarray(format='bgr24')\n",
    "            img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # rebuild a VideoFrame, preserving timing information\n",
    "            new_frame = VideoFrame.from_ndarray(img, format='bgr24')\n",
    "            new_frame.pts = frame.pts\n",
    "            new_frame.time_base = frame.time_base\n",
    "            return new_frame\n",
    "        elif self.transform == 'rotate':\n",
    "            # rotate image\n",
    "            img = frame.to_ndarray(format='bgr24')\n",
    "            rows, cols, _ = img.shape\n",
    "            M = cv2.getRotationMatrix2D((cols / 2, rows / 2), frame.time * 45, 1)\n",
    "            img = cv2.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "            # rebuild a VideoFrame, preserving timing information\n",
    "            new_frame = VideoFrame.from_ndarray(img, format='bgr24')\n",
    "            new_frame.pts = frame.pts\n",
    "            new_frame.time_base = frame.time_base\n",
    "            return new_frame\n",
    "        else:\n",
    "            return frame\n",
    "\n",
    "class VideoImageTrack(VideoStreamTrack):\n",
    "    \"\"\"\n",
    "    A video stream track that returns a rotating image.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()  # don't forget this!\n",
    "        self.img = cv2.imread(PHOTO_PATH, cv2.IMREAD_COLOR)\n",
    "\n",
    "    async def recv(self):\n",
    "        pts, time_base = await self.next_timestamp()\n",
    "\n",
    "        # rotate image\n",
    "        rows, cols, _ = self.img.shape\n",
    "        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), int(pts * time_base * 45), 1)\n",
    "        img = cv2.warpAffine(self.img, M, (cols, rows))\n",
    "\n",
    "        # create video frame\n",
    "        frame = VideoFrame.from_ndarray(img, format='bgr24')\n",
    "        frame.pts = pts\n",
    "        frame.time_base = time_base\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "async def run(pc, player, recorder, signaling):\n",
    "    def add_tracks():\n",
    "        \n",
    "      #if player and player.audio:\n",
    "      #    pc.addTrack(player.audio)\n",
    "\n",
    "      if player and player.video:\n",
    "          local_video = player.video\n",
    "          local_video = VideoTransformTrack(player.video, transform='edges')\n",
    "          #local_video = VideoTransformTrack(player.video, transform='cartoon')\n",
    "          pc.addTrack(local_video)\n",
    "      else:\n",
    "          pc.addTransceiver('video','sendrecv') #this is the trick to echo webcame back\n",
    "          #pc.addTrack(local_video) #VideoImageTrack())\n",
    "\n",
    "#     @pc.on('track')\n",
    "#     def on_track(track):\n",
    "#         print('Track %s received' % track.kind)\n",
    "#         recorder.addTrack(track)\n",
    "\n",
    "    @pc.on('track')\n",
    "    def on_track(track):\n",
    "        #log_info('Track %s received', track.kind)\n",
    "        print('Track %s received' % track.kind)\n",
    "        if track.kind == 'audio':\n",
    "            #pc.addTrack(player.audio)\n",
    "            recorder.addTrack(track)\n",
    "        elif track.kind == 'video':\n",
    "            remote_video = track\n",
    "            remote_video = VideoTransformTrack(track, transform='edges')\n",
    "            #remote_video = VideoTransformTrack(track, transform='rotate')\n",
    "            pc.addTrack(remote_video)\n",
    "            recorder.addTrack(remote_video)\n",
    "\n",
    "        @track.on('ended')\n",
    "        async def on_ended():\n",
    "            #log_info('Track %s ended', track.kind)\n",
    "            await recorder.stop()\n",
    "\n",
    "\n",
    "    # connect to websocket and join\n",
    "    params = await signaling.connect()\n",
    "\n",
    "    if params['is_initiator'] == 'true':\n",
    "        # send offer\n",
    "        add_tracks()\n",
    "        await pc.setLocalDescription(await pc.createOffer())\n",
    "        await signaling.send(pc.localDescription)\n",
    "        print('Please point a browser at %s' % params['room_link'])\n",
    "\n",
    "    # consume signaling\n",
    "    while True:\n",
    "        obj = await signaling.receive()\n",
    "\n",
    "        if isinstance(obj, RTCSessionDescription):\n",
    "            await pc.setRemoteDescription(obj)\n",
    "            await recorder.start()\n",
    "\n",
    "            if obj.type == 'offer':\n",
    "                # send answer\n",
    "                add_tracks()\n",
    "                \n",
    "#                 print('Add processed_video track')\n",
    "#                 processed_video = VideoTransformTrack(pc.getSenders()[0].track, transform='edges')\n",
    "#                 pc.addTrack(processed_video) #VideoImageTrack())\n",
    "\n",
    "                await pc.setLocalDescription(await pc.createAnswer())\n",
    "                await signaling.send(pc.localDescription)\n",
    "        elif isinstance(obj, RTCIceCandidate):\n",
    "            pc.addIceCandidate(obj)\n",
    "        else:\n",
    "            print('Exiting')\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='AppRTC')\n",
    "#     parser.add_argument('room', nargs='?')\n",
    "#     parser.add_argument('--play-from', help='Read the media from a file and sent it.'),\n",
    "#     parser.add_argument('--record-to', help='Write received media to a file.'),\n",
    "#     parser.add_argument('--verbose', '-v', action='count')\n",
    "#     args = parser.parse_args()\n",
    "  \n",
    "    args_room = None #\"test1234test1\"\n",
    "    args_play_from = None\n",
    "    args_play_from = 'video6.mp4'\n",
    "    args_play_from = 'colorbars.mp4'\n",
    "    #args_play_from = 'video14-best.mp4'\n",
    "    args_play_from = None\n",
    "    args_record_to = None\n",
    "    #args_record_to = '/content/video7.mp4'\n",
    "    \n",
    "    if not args_room:\n",
    "        args_room = ''.join([random.choice('0123456789') for x in range(10)])\n",
    "\n",
    "#     if args_verbose:\n",
    "#         logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "     # create signaling and peer connection\n",
    "    signaling = ApprtcSignaling(args_room)\n",
    "    pc = RTCPeerConnection()\n",
    "\n",
    "    # create media source\n",
    "    if args_play_from:\n",
    "        player = MediaPlayer(args_play_from)\n",
    "    else:\n",
    "        player = None\n",
    "\n",
    "    # create media sink\n",
    "    if args_record_to:\n",
    "        recorder = MediaRecorder(args_record_to)\n",
    "    else:\n",
    "        recorder = MediaBlackhole()\n",
    "\n",
    "    # run event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(run(\n",
    "            pc=pc,\n",
    "            player=player,\n",
    "            recorder=recorder,\n",
    "            signaling=signaling))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        # cleanup\n",
    "        loop.run_until_complete(recorder.stop())\n",
    "        loop.run_until_complete(signaling.close())\n",
    "        loop.run_until_complete(pc.close())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0-pre) /data/data/com.termux/files/home/build/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-51483170058c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#image = vid.reader.get_next_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#clientsocket.send(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.0-pre) /data/data/com.termux/files/home/build/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import pylab\n",
    "import imageio\n",
    "import cv2, pafy\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=uuQlMCMT71I\"\n",
    "#url = \"https://www.youtube.com/watch?v=OHKAkhdQ6B4\"\n",
    "videoPafy = pafy.new(url)\n",
    "best = videoPafy.getbest(preftype=\"webm\")\n",
    "best = videoPafy.getbest(preftype=\"mp4\")\n",
    "\n",
    "#imageio.setUseCache(false)\n",
    "\n",
    "#on server run this command:\n",
    "#ffplay -f mjpeg tcp://0.0.0.0:45654?listen\n",
    "#clientsocket=socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "#clientsocket.connect((ip,45654)) #the target ip is where the ffplay is listening\n",
    "\n",
    "#filename = 'http://techslides.com/demos/sample-videos/small.mp4'\n",
    "filename = best.url\n",
    "#vid = imageio.get_reader(filename,  'ffmpeg')\n",
    "nums = range(1,1000)\n",
    "\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "#for image in vid:\n",
    "#for i, image in enumerate(vid):\n",
    "for num in nums:\n",
    "    flag, image = cap.retrieve()   \n",
    "    #image = vid.reader.get_next_data()\n",
    "    #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    data = cv2.imencode('.jpg', gray)[1].tobytes()\n",
    "    #clientsocket.send(data)\n",
    "    fig = pylab.figure()\n",
    "    fig.suptitle('image #{}'.format(num), fontsize=20)\n",
    "    pylab.imshow(gray)\n",
    "    pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
